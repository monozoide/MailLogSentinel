# -*- coding: utf-8 -*-
"""
Handles the SQL import functionality for MailLogSentinel.

Scans for .sql files generated by the export process, imports them into the
SQLite3 database, manages concurrency with a lock file, and handles errors
with retries and rollbacks.
"""

import logging
import sqlite3
import time
import datetime  # For handling imported file records
import os  # For os.getpid() in FileLock
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
import fcntl  # For file locking on POSIX systems

import importlib.resources  # Added for loading bundled data

# Assuming AppConfig will be passed, similar to sql_exporter
from lib.maillogsentinel.config import AppConfig
from lib.maillogsentinel.sql_exporter import (
    load_column_mapping,
)  # Re-use for table creation

logger = logging.getLogger(__name__)
LOG_PREFIX = "sql_import"  # General prefix for the import process
LOG_PREFIX_DB = "sql_db"  # Specific prefix for database interactions
SQL_DIR_NAME = "sql"  # Subdirectory in working_dir for .sql files
LOCK_FILENAME = "import.lock"
IMPORTED_FILES_LOG = "sql_imported_files.log"  # In state_dir

# --- Retry constants ---
MAX_RETRIES = 5
INITIAL_BACKOFF_SECONDS = 1
MAX_BACKOFF_SECONDS = 60


class LockError(Exception):
    """Custom exception for lock acquisition failures."""

    pass


class DatabaseError(Exception):
    """Custom exception for database operation failures."""

    pass


class FileLock:
    """
    A simple file-based lock for POSIX systems using fcntl.
    Ensures only one instance of the import process runs.
    """

    def __init__(self, lock_file_path: Path):
        self.lock_file_path = lock_file_path
        self._lock_file_handle = None

    def acquire(self) -> bool:
        """
        Acquires the lock. Returns True if successful, False otherwise.
        """
        logger.debug(f"{LOG_PREFIX}: Attempting to acquire lock: {self.lock_file_path}")
        try:
            self.lock_file_path.parent.mkdir(parents=True, exist_ok=True)
            # Open in append mode so it doesn't truncate if it exists; content doesn't matter.
            self._lock_file_handle = open(self.lock_file_path, "a")
            fcntl.flock(self._lock_file_handle.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            logger.info(f"{LOG_PREFIX}: Lock acquired: {self.lock_file_path}")
            # Write PID to lock file for informational purposes
            self._lock_file_handle.seek(0)
            self._lock_file_handle.truncate()
            self._lock_file_handle.write(
                str(Path.cwd().owner()) + "\n"
            )  # For debugging who owns it
            self._lock_file_handle.write(str(os.getpid()) + "\n")
            self._lock_file_handle.flush()
            return True
        except (IOError, BlockingIOError):
            logger.warning(
                f"{LOG_PREFIX}: Lock {self.lock_file_path} is already held by another process."
            )
            if self._lock_file_handle:
                self._lock_file_handle.close()
                self._lock_file_handle = None
            return False
        except Exception as e:
            logger.error(
                f"{LOG_PREFIX}: Error acquiring lock {self.lock_file_path}: {e}",
                exc_info=True,
            )
            if self._lock_file_handle:
                self._lock_file_handle.close()
                self._lock_file_handle = None
            # Re-raise as a LockError for clarity in the calling function
            raise LockError(f"Failed to acquire lock: {e}")

    def release(self) -> None:
        """
        Releases the lock.
        """
        if self._lock_file_handle:
            logger.debug(f"{LOG_PREFIX}: Releasing lock: {self.lock_file_path}")
            fcntl.flock(self._lock_file_handle.fileno(), fcntl.LOCK_UN)
            self._lock_file_handle.close()
            self._lock_file_handle = None
            # Optionally remove the lock file, though not strictly necessary
            # self.lock_file_path.unlink(missing_ok=True)
            logger.info(f"{LOG_PREFIX}: Lock released: {self.lock_file_path}")

    def __enter__(self):
        if not self.acquire():
            raise LockError(f"Could not acquire lock: {self.lock_file_path}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()


def get_db_connection(db_path: Path) -> sqlite3.Connection:
    """
    Establishes a connection to the SQLite database.

    Args:
        db_path: Path to the SQLite database file.

    Returns:
        An sqlite3.Connection object.

    Raises:
        DatabaseError: If the connection cannot be established.
    """
    logger.debug(f"{LOG_PREFIX_DB}: Connecting to database: {db_path}")
    try:
        db_path.parent.mkdir(parents=True, exist_ok=True)
        conn = sqlite3.connect(str(db_path), timeout=10)  # Increased timeout
        logger.info(f"{LOG_PREFIX_DB}: Successfully connected to database: {db_path}")
        return conn
    except sqlite3.Error as e:
        logger.error(
            f"{LOG_PREFIX_DB}: Failed to connect to database {db_path}: {e}",
            exc_info=True,
        )
        raise DatabaseError(f"Could not connect to SQLite database: {e}")


def create_table_if_not_exists(
    conn: sqlite3.Connection, table_name: str, column_mapping_file: Path
) -> None:
    """
    Creates the target table in the database if it doesn't already exist,
    based on the SQL column definitions in the mapping file.

    Args:
        conn: Active SQLite database connection.
        table_name: Name of the table to create.
        column_mapping_file: Path to the JSON column mapping file.

    Raises:
        DatabaseError: If table creation fails.
    """
    logger.info(f"{LOG_PREFIX_DB}: Ensuring table '{table_name}' exists...")
    try:
        # load_column_mapping is from sql_exporter, its internal logs will use its own prefix.
        mapping = load_column_mapping(column_mapping_file)

        # Construct CREATE TABLE statement
        # Example: "id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY" becomes "id INTEGER PRIMARY KEY AUTOINCREMENT" for SQLite
        # SQLite types are a bit different: TEXT, INTEGER, REAL, BLOB, NULL
        # AUTO_INCREMENT is handled by INTEGER PRIMARY KEY AUTOINCREMENT
        # VARCHAR(N) becomes TEXT
        # DATETIME becomes TEXT (ISO8601 strings recommended)
        # ENUM becomes TEXT with a CHECK constraint

        column_defs_sql = []
        for sql_col_name, info in mapping.items():
            sql_def = info.get("sql_column_def", "").upper()
            sqlite_col_def = sql_col_name  # Start with name

            if (
                "PRIMARY KEY AUTO_INCREMENT" in sql_def
                or "AUTO_INCREMENT PRIMARY KEY" in sql_def
            ):
                # SQLite specific for auto-incrementing primary key
                sqlite_col_def += " INTEGER PRIMARY KEY AUTOINCREMENT"
            else:
                if "INT" in sql_def:
                    sqlite_col_def += " INTEGER"
                elif "CHAR" in sql_def:
                    sqlite_col_def += " TEXT"  # VARCHAR, CHAR -> TEXT
                elif "TEXT" in sql_def:
                    sqlite_col_def += " TEXT"
                elif "DATETIME" in sql_def:
                    sqlite_col_def += " TEXT"  # Store as ISO8601 strings
                elif "ENUM" in sql_def:
                    sqlite_col_def += " TEXT"
                    # Extract enum values for CHECK constraint, e.g., ENUM('OK', 'Errno 1')
                    try:
                        enum_values_str = sql_def.split("ENUM(")[1].split(")")[0]
                        enum_values = [
                            val.strip().strip("'") for val in enum_values_str.split(",")
                        ]
                        check_values = ", ".join([f"'{v}'" for v in enum_values])
                        sqlite_col_def += f" CHECK ({sql_col_name} IN ({check_values}))"
                    except IndexError:
                        logger.warning(
                            f"{LOG_PREFIX_DB}: Could not parse ENUM values for {sql_col_name} from '{sql_def}'. No CHECK constraint added."
                        )
                else:  # Default to TEXT for unknown types
                    sqlite_col_def += " TEXT"

                if "NOT NULL" in sql_def:
                    sqlite_col_def += " NOT NULL"
                if "DEFAULT NULL" in sql_def:
                    pass  # SQLite columns are nullable by default unless NOT NULL is specified
                elif "DEFAULT " in sql_def:  # e.g. DEFAULT 'some_value' or DEFAULT 0
                    default_val_str = sql_def.split("DEFAULT ")[1].split(" ")[
                        0
                    ]  # Basic parse
                    sqlite_col_def += f" DEFAULT {default_val_str}"  # Assumes default_val_str is correctly quoted if string

            column_defs_sql.append(sqlite_col_def)

        create_table_sql = (
            f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(column_defs_sql)});"
        )
        logger.debug(
            f"{LOG_PREFIX_DB}: Executing CREATE TABLE statement: {create_table_sql}"
        )

        cursor = conn.cursor()
        cursor.execute(create_table_sql)
        conn.commit()
        logger.info(f"{LOG_PREFIX_DB}: Table '{table_name}' ensured to exist.")
    except sqlite3.Error as e:
        logger.error(
            f"{LOG_PREFIX_DB}: Database error ensuring table '{table_name}' exists: {e}",
            exc_info=True,
        )
        raise DatabaseError(f"Failed to create/verify table '{table_name}': {e}")
    except Exception as e:  # Catch errors from load_column_mapping or parsing
        logger.error(
            f"{LOG_PREFIX_DB}: Error preparing CREATE TABLE statement for '{table_name}': {e}",
            exc_info=True,
        )
        raise DatabaseError(f"Could not prepare CREATE TABLE for '{table_name}': {e}")


def get_already_imported_files(imported_log_path: Path) -> set[str]:
    """Reads the list of already imported .sql file basenames."""
    if not imported_log_path.is_file():
        return set()
    try:
        with open(imported_log_path, "r", encoding="utf-8") as f:
            return {line.strip() for line in f if line.strip()}
    except IOError as e:
        logger.warning(
            f"{LOG_PREFIX}: Could not read imported files log {imported_log_path}: {e}. Assuming no files imported via this log.",
            exc_info=True,
        )
        return set()


def mark_file_as_imported(imported_log_path: Path, sql_file_basename: str) -> None:
    """Appends a successfully imported .sql file basename to the log."""
    try:
        imported_log_path.parent.mkdir(parents=True, exist_ok=True)
        with open(imported_log_path, "a", encoding="utf-8") as f:
            f.write(sql_file_basename + "\n")
    except IOError as e:
        logger.error(
            f"{LOG_PREFIX}: Failed to mark file {sql_file_basename} as imported in {imported_log_path}: {e}",
            exc_info=True,
        )
        # This is serious, as it could lead to re-importing.


def run_sql_import(config: AppConfig, output_log_level: str = "INFO") -> bool:
    """
    Main function to perform the SQL import process.

    Args:
        config: The application configuration object (AppConfig).
        output_log_level: The desired log level for console output.

    Returns:
        True if import completed successfully (or no new files), False otherwise.
    """
    logger.info(f"{LOG_PREFIX}: Starting SQL import process.")

    lock_file = config.state_dir / LOCK_FILENAME
    sql_files_dir = config.working_dir / SQL_DIR_NAME
    db_path = Path(config.sqlite_db_path)  # Ensure it's a Path object
    imported_log_path = config.state_dir / IMPORTED_FILES_LOG
    table_name = (
        config.sql_target_table_name
    )  # Moved up, as it's independent of mapping path

    # --- Determine Column Mapping File Path ---
    raw_mapping_path_str = config.sql_column_mapping_file_path_str
    final_mapping_file_path: Optional[Path] = None
    loaded_mapping_source_log_msg = ""

    if raw_mapping_path_str and raw_mapping_path_str.strip():
        logger.info(
            f"{LOG_PREFIX}: User-specified column mapping file path: '{raw_mapping_path_str}'"
        )
        user_mapping_file_p = Path(raw_mapping_path_str)
        if not user_mapping_file_p.is_absolute():
            if config.config_path and config.config_path.is_file():
                user_mapping_file_p = config.config_path.parent / raw_mapping_path_str
                logger.debug(
                    f"{LOG_PREFIX}: Resolved relative user mapping path to: {user_mapping_file_p} (relative to config file {config.config_path})"
                )
            else:
                user_mapping_file_p = Path.cwd() / raw_mapping_path_str
                logger.debug(
                    f"{LOG_PREFIX}: Resolved relative user mapping path to: {user_mapping_file_p} (relative to CWD)"
                )

        final_mapping_file_path = user_mapping_file_p.resolve()
        loaded_mapping_source_log_msg = (
            f"user-specified file: {final_mapping_file_path}"
        )
        # Actual loading and FileNotFoundError for this user-specified path will be handled by create_table_if_not_exists
        # which calls load_column_mapping. If it fails there, it's a critical error for the import.
    else:
        logger.info(
            f"{LOG_PREFIX}: No user-specific column mapping file configured, attempting to load bundled default."
        )
        try:
            with importlib.resources.files("lib.maillogsentinel.data").joinpath(
                "maillogsentinel_sql_column_mapping.json"
            ) as bundled_path_traversable:
                final_mapping_file_path = Path(
                    bundled_path_traversable
                )  # Convert Traversable to Path
                if not final_mapping_file_path.is_file():
                    logger.critical(
                        f"{LOG_PREFIX}: Bundled column mapping file not found at expected location via importlib.resources: {final_mapping_file_path}. This indicates a packaging issue. Aborting SQL import."
                    )
                    return False  # Abort early if bundled default is missing
                loaded_mapping_source_log_msg = f"bundled default: lib.maillogsentinel.data/maillogsentinel_sql_column_mapping.json (resolved to {final_mapping_file_path})"
        except ModuleNotFoundError:  # If lib.maillogsentinel.data is not a package
            logger.critical(
                f"{LOG_PREFIX}: Could not find the 'lib.maillogsentinel.data' package for bundled resources. This indicates a packaging issue. Aborting SQL import.",
                exc_info=True,
            )
            return False
        except (
            FileNotFoundError
        ):  # Should be caught by is_file() check, but as safeguard
            logger.critical(
                f"{LOG_PREFIX}: Bundled column mapping file not found via importlib.resources (FileNotFoundError). This indicates a packaging issue. Aborting SQL import.",
                exc_info=True,
            )
            return False
        except Exception as e:
            logger.critical(
                f"{LOG_PREFIX}: Error loading bundled default column mapping file: {e}. Aborting SQL import.",
                exc_info=True,
            )
            return False

    if (
        final_mapping_file_path is None
    ):  # Safeguard, should have been set or returned False
        logger.critical(
            f"{LOG_PREFIX}: Column mapping file path could not be determined. Aborting SQL import."
        )
        return False

    logger.info(
        f"{LOG_PREFIX}: Using column mapping from {loaded_mapping_source_log_msg} for table creation."
    )
    # --- End Determine Column Mapping File Path ---

    importer_lock = FileLock(lock_file)
    try:
        with importer_lock:  # Acquires lock, raises LockError if fails
            logger.info(f"{LOG_PREFIX}: Lock acquired. Proceeding with import.")

            conn: Optional[sqlite3.Connection] = None
            retries = 0
            while retries <= MAX_RETRIES:
                try:
                    conn = get_db_connection(db_path)
                    # Ensure table exists
                    create_table_if_not_exists(
                        conn, table_name, final_mapping_file_path
                    )
                    break  # Connection successful
                except (
                    DatabaseError
                ) as e:  # Errors here will have LOG_PREFIX_DB from underlying functions
                    logger.error(
                        f"{LOG_PREFIX}: Database connection or initial table setup error: {e}"
                    )  # General error for the step
                    if retries == MAX_RETRIES:
                        logger.critical(
                            f"{LOG_PREFIX_DB}: Max retries reached for DB connection/setup. Aborting."
                        )
                        return False

                    backoff_time = min(
                        INITIAL_BACKOFF_SECONDS * (2**retries), MAX_BACKOFF_SECONDS
                    )
                    logger.info(
                        f"{LOG_PREFIX_DB}: Retrying DB connection/setup in {backoff_time} seconds... (Attempt {retries + 1}/{MAX_RETRIES})"
                    )
                    time.sleep(backoff_time)
                    retries += 1

            if not conn:  # Should not happen if loop logic is correct
                logger.critical(
                    f"{LOG_PREFIX_DB}: Failed to establish database connection after retries."
                )
                return False

            already_imported = get_already_imported_files(imported_log_path)

            if not sql_files_dir.exists() or not any(sql_files_dir.iterdir()):
                logger.info(
                    f"{LOG_PREFIX}: SQL files directory {sql_files_dir} is empty or does not exist. No files to import."
                )
                if conn:
                    conn.close()
                return True

            sql_files_to_import = sorted(
                [
                    f
                    for f in sql_files_dir.glob("*.sql")
                    if f.is_file() and f.name not in already_imported
                ]
            )

            if not sql_files_to_import:
                logger.info(
                    f"{LOG_PREFIX}: No new SQL files found to import in {sql_files_dir}."
                )
                if conn:
                    conn.close()
                return True

            logger.info(
                f"{LOG_PREFIX}: Found {len(sql_files_to_import)} new SQL file(s) to import."
            )
            total_files_processed = 0
            total_files_failed = 0

            for sql_file in sql_files_to_import:
                logger.info(f"{LOG_PREFIX}: Processing SQL file: {sql_file.name}")
                try:
                    with open(sql_file, "r", encoding="utf-8") as f_sql:
                        sql_script = f_sql.read()

                    # SQLite's executescript handles transactions within the script.
                    # If the script has BEGIN/COMMIT, it will be atomic.
                    # If not, executescript itself doesn't wrap in a transaction by default.
                    # Our exporter creates files with BEGIN TRANSACTION; ... COMMIT;
                    conn.executescript(
                        sql_script
                    )  # This will commit if successful, or rollback if error within script
                    conn.commit()  # Ensure changes from executescript are committed if it doesn't do it itself for some reason (it should)

                    mark_file_as_imported(imported_log_path, sql_file.name)
                    logger.info(
                        f"{LOG_PREFIX}: Successfully imported {sql_file.name}"
                    )  # General import log
                    total_files_processed += 1
                except sqlite3.Error as e_db:
                    logger.error(
                        f"{LOG_PREFIX_DB}: SQLite error processing file {sql_file.name}: {e_db}. The transaction in the file should have rolled back.",
                        exc_info=True,
                    )
                    # Automatic rollback on error is a feature of how SQLite handles executescript with transactional SQL files.
                    # No explicit rollback call needed here if the .sql file itself is transactional.
                    total_files_failed += 1
                except (
                    IOError
                ) as e_io:  # This is an I/O error for reading the .sql file, not DB specific
                    logger.error(
                        f"{LOG_PREFIX}: IO error reading file {sql_file.name}: {e_io}",
                        exc_info=True,
                    )
                    total_files_failed += 1
                except Exception as e_gen:  # General error, could be anything
                    logger.error(
                        f"{LOG_PREFIX}: Unexpected error processing file {sql_file.name}: {e_gen}",
                        exc_info=True,
                    )
                    total_files_failed += 1

            if conn:
                logger.debug(f"{LOG_PREFIX_DB}: Closing database connection.")
                conn.close()

            logger.info(
                f"{LOG_PREFIX}: SQL import process finished. Processed: {total_files_processed} files. Failed: {total_files_failed} files."
            )
            return total_files_failed == 0

    except LockError:
        logger.warning(
            f"{LOG_PREFIX}: Could not acquire import lock. Another import process may be running. Exiting."
        )  # Lock related, uses general LOG_PREFIX
        return False  # Indicate failure due to locking
    except Exception as e:
        logger.critical(
            f"{LOG_PREFIX}: Unhandled exception during SQL import: {e}", exc_info=True
        )  # General import error
        return False
    finally:
        # Ensure lock is released if acquired and an unexpected error occurred before __exit__
        if importer_lock._lock_file_handle:  # Check if lock was acquired
            importer_lock.release()


if __name__ == "__main__":
    # Basic testing for sql_importer.py
    print("Running sql_importer.py directly for testing.")
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    import os  # For os.getpid() in FileLock

    # Dummy AppConfig for testing
    class DummyConfigForImport:
        def __init__(self, base_path: Path):
            self.test_base_path = base_path
            self.working_dir = self.test_base_path / "working"
            self.state_dir = self.test_base_path / "state"
            self.sqlite_db_path = self.state_dir / "maillogsentinel_test.sqlite"

            # Config path for resolving relative mapping file
            self.config_path = self.test_base_path / "config_dir" / "dummy.conf"
            self.sql_column_mapping_file_path_str = "../config/maillogsentinel_sql_column_mapping.json"  # Relative to config_path.parent
            self.sql_target_table_name = "maillog_events_imported"

            # Ensure directories for dummy config exist
            self.working_dir.mkdir(parents=True, exist_ok=True)
            (self.working_dir / SQL_DIR_NAME).mkdir(parents=True, exist_ok=True)
            self.state_dir.mkdir(parents=True, exist_ok=True)
            self.config_path.parent.mkdir(parents=True, exist_ok=True)

            # Create dummy config file if it doesn't exist
            if not self.config_path.exists():
                with open(self.config_path, "w") as f:
                    f.write("#dummy\n")

            # Create the actual column mapping file if it doesn't exist at the expected resolved location
            # This is needed for create_table_if_not_exists
            resolved_mapping_path = (
                self.config_path.parent / self.sql_column_mapping_file_path_str
            )
            resolved_mapping_path.parent.mkdir(
                parents=True, exist_ok=True
            )  # Ensure 'config' subdir exists
            if not resolved_mapping_path.exists():
                print(
                    f"Creating dummy mapping file at {resolved_mapping_path} for test"
                )
                with open(resolved_mapping_path, "w") as mf:
                    # A minimal valid mapping for testing table creation
                    # Based on the one from sql_exporter, but simplified for sqlite
                    # This should match the structure expected by load_column_mapping
                    # and create_table_if_not_exists's SQLite conversion
                    dummy_map_content = {
                        "id": {
                            "sql_column_def": "INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY"
                        },
                        "server": {"sql_column_def": "VARCHAR(50) NOT NULL"},
                        "event_time": {"sql_column_def": "DATETIME NOT NULL"},
                        "ip": {"sql_column_def": "VARCHAR(45) NOT NULL"},
                        "username": {"sql_column_def": "VARCHAR(100) NOT NULL"},
                        "hostname": {"sql_column_def": "VARCHAR(255) DEFAULT NULL"},
                        "reverse_dns_status": {
                            "sql_column_def": "ENUM('OK', 'ERRNO 1', 'ERRNO 2', 'ERRNO 3', 'ERRNO 4') NOT NULL"
                        },
                        "country_code": {"sql_column_def": "CHAR(2) NOT NULL"},
                        "asn_int": {"sql_column_def": "INT UNSIGNED NOT NULL"},
                        "asn_varchar": {"sql_column_def": "VARCHAR(255) NOT NULL"},
                    }
                    import json

                    json.dump(dummy_map_content, mf)

    # Create a temporary directory for this test run
    import tempfile

    with tempfile.TemporaryDirectory(prefix="mls_importer_test_") as tmpdir_name:
        tmp_base_path = Path(tmpdir_name)
        print(f"Test workspace: {tmp_base_path}")

        test_config = DummyConfigForImport(tmp_base_path)

        # Clean up previous test artifacts if any (db, lock, imported log)
        if test_config.sqlite_db_path.exists():
            test_config.sqlite_db_path.unlink()
        lock_f = test_config.state_dir / LOCK_FILENAME
        if lock_f.exists():
            lock_f.unlink()
        imported_log_f = test_config.state_dir / IMPORTED_FILES_LOG
        if imported_log_f.exists():
            imported_log_f.unlink()

        # Create some dummy .sql files
        sql_dir = test_config.working_dir / SQL_DIR_NAME

        # File 1: Valid SQL
        with open(sql_dir / "20230101_1000_export.sql", "w") as f:
            f.write("BEGIN TRANSACTION;\n")
            # Use table name from test_config
            f.write(
                f"INSERT INTO {test_config.sql_target_table_name} (server, event_time, ip, username, hostname, reverse_dns_status, country_code, asn_int, asn_varchar) VALUES ('srv1', '2023-01-01 10:00:00', '1.1.1.1', 'user1', 'host1', 'OK', 'US', 100, 'AS100 ISP');\n"
            )
            f.write("COMMIT;\n")

        # File 2: Valid SQL, different data
        with open(sql_dir / "20230101_1100_export.sql", "w") as f:
            f.write("BEGIN TRANSACTION;\n")
            f.write(
                f"INSERT INTO {test_config.sql_target_table_name} (server, event_time, ip, username, hostname, reverse_dns_status, country_code, asn_int, asn_varchar) VALUES ('srv2', '2023-01-01 11:00:00', '2.2.2.2', 'user2', 'host2', 'ERRNO 1', 'CA', 200, 'AS200 ISP');\n"
            )
            f.write("COMMIT;\n")

        # File 3: Invalid SQL (e.g., syntax error)
        with open(sql_dir / "20230101_1200_export_error.sql", "w") as f:
            f.write("BEGIN TRANSACTION;\n")
            f.write(
                f"INSERT INTO {test_config.sql_target_table_name} (server, event_time, ip, username) VALUES ('srv3', '2023-01-01 12:00:00', '3.3.3.3', ); -- Syntax error here \n"
            )  # Invalid SQL
            f.write("COMMIT;\n")

        print("\n--- Running SQL Import (1st time) ---")
        success1 = run_sql_import(test_config)
        print(f"Import 1 success: {success1}")  # Should be False due to error file

        # Verify DB content for srv1, srv2 (srv3 should not be there if transaction rolled back)
        if test_config.sqlite_db_path.exists():
            conn_check = sqlite3.connect(test_config.sqlite_db_path)
            cursor_check = conn_check.cursor()
            try:
                cursor_check.execute(
                    f"SELECT COUNT(*) FROM {test_config.sql_target_table_name}"
                )
                count = cursor_check.fetchone()[0]
                print(
                    f"Records in DB after 1st import: {count}"
                )  # Expect 2 if error file failed correctly
                cursor_check.execute(
                    f"SELECT username FROM {test_config.sql_target_table_name} WHERE server = 'srv1'"
                )
                print(f"User for srv1: {cursor_check.fetchone()}")
                cursor_check.execute(
                    f"SELECT username FROM {test_config.sql_target_table_name} WHERE server = 'srv3'"
                )
                print(
                    f"User for srv3: {cursor_check.fetchone()} (should be None if error file failed)"
                )
            except sqlite3.Error as e:
                print(f"Error checking DB: {e}")
            finally:
                conn_check.close()
        else:
            print("DB file does not exist after 1st import.")

        print("\n--- Running SQL Import (2nd time) ---")
        # Should process no new files if error file is not renamed/removed
        # If error file was marked imported (bad), or if it's ignored, it will say no new files.
        # Current logic: failed files are not marked imported. So error file will be re-attempted.
        success2 = run_sql_import(test_config)
        print(f"Import 2 success: {success2}")  # Still False

        # Manually mark the error file as imported to simulate it being "fixed" or ignored
        # Or rename it. For this test, let's update the imported log.
        if imported_log_f.exists():
            mark_file_as_imported(imported_log_f, "20230101_1200_export_error.sql")
            print("Manually marked error file as imported for next test run.")

        print("\n--- Running SQL Import (3rd time, after error file 'fixed') ---")
        success3 = run_sql_import(test_config)  # Should be True now
        print(f"Import 3 success: {success3}")

        # Test locking: try to run it again while it (theoretically) could be running
        # This is hard to test without true concurrency.
        # We can simulate by trying to acquire lock if another instance holds it.
        print("\n--- Testing Lock File ---")
        # If run_sql_import is called again, it should fail to get lock if the first one didn't release it
        # (e.g. due to crash). Our context manager should handle release.
        # This test is more about showing the lock can be acquired again.
        # If another process had the lock, it would print "Lock ... is already held".
        if lock_f.exists():
            lock_f.unlink()  # Clean previous lock
        if imported_log_f.exists():
            imported_log_f.unlink()  # Clean imported log
        # Create a new file to import
        with open(sql_dir / "20230101_1300_export.sql", "w") as f:
            f.write("BEGIN TRANSACTION;\n")
            f.write(
                f"INSERT INTO {test_config.sql_target_table_name} (server, event_time, ip, username) VALUES ('srv4', '2023-01-01 13:00:00', '4.4.4.4', 'user4');\n"
            )
            f.write("COMMIT;\n")

        success_lock_test = run_sql_import(test_config)
        print(f"Lock test import success: {success_lock_test}")

    print("\nsql_importer.py direct test finished.")
    # Note: For a real test, you'd use pytest and mock AppConfig, Path.is_file(), open(), etc.
    # This direct run is just for very basic flow checking.

# Required for FileLock to get PID (if not already imported by another module like logging)
import os
